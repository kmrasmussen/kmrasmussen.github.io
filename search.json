[
  {
    "objectID": "posts/jaxadventure1/index.html",
    "href": "posts/jaxadventure1/index.html",
    "title": "jax adventures 001",
    "section": "",
    "text": "I have been wanting to learn more Jax, so I will go on an a Jax adventure.\n\nimport jax\nimport jax.numpy as jnp\nimport flax.linen as nn\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nThese imports should be sufficient for what I’d need.\nI saw there is some kind of selfattention module in flax. Apparently Flax Linen is being taken over by Flax NNX\n\nfrom flax import nnx\nmy_selfattention = nnx.MultiHeadAttention(\n  num_heads=2,\n  in_features=10,\n  qkv_features=6,\n  out_features=12,\n  decode=False,\n  rngs=nnx.Rngs(0)\n)\n\nOkay now I have this object, I assume it is a nnx module. Usually then it has to be initialized with parameters separately, but why is there then an rngs associated with it?\n\nprint(my_selfattention)\n\n\nMultiHeadAttention( # Param: 282 (1.1 KB)\n\n  num_heads=2,\n\n  in_features=10,\n\n  qkv_features=6,\n\n  out_features=12,\n\n  dtype=None,\n\n  param_dtype=float32,\n\n  broadcast_dropout=True,\n\n  dropout_rate=0.0,\n\n  deterministic=None,\n\n  precision=None,\n\n  kernel_init=&lt;function variance_scaling.&lt;locals&gt;.init at 0x73e5c28e2700&gt;,\n\n  out_kernel_init=None,\n\n  bias_init=&lt;function zeros at 0x73e5c362c5e0&gt;,\n\n  out_bias_init=None,\n\n  use_bias=True,\n\n  attention_fn=&lt;function dot_product_attention at 0x73e5c28e2200&gt;,\n\n  decode=False,\n\n  normalize_qk=False,\n\n  qkv_dot_general=None,\n\n  out_dot_general=None,\n\n  qkv_dot_general_cls=None,\n\n  out_dot_general_cls=None,\n\n  head_dim=3,\n\n  query=LinearGeneral( # Param: 66 (264 B)\n\n    in_features=(10,),\n\n    out_features=(2, 3),\n\n    axis=(-1,),\n\n    batch_axis=FrozenDict({}),\n\n    use_bias=True,\n\n    dtype=None,\n\n    param_dtype=float32,\n\n    kernel_init=&lt;function variance_scaling.&lt;locals&gt;.init at 0x73e5c28e2700&gt;,\n\n    bias_init=&lt;function zeros at 0x73e5c362c5e0&gt;,\n\n    precision=None,\n\n    dot_general=None,\n\n    dot_general_cls=None,\n\n    promote_dtype=&lt;function promote_dtype at 0x73e5c28e2160&gt;,\n\n    kernel=Param( # 60 (240 B)\n\n      value=Array(shape=(10, 2, 3), dtype=dtype('float32'))\n\n    ),\n\n    bias=Param( # 6 (24 B)\n\n      value=Array(shape=(2, 3), dtype=dtype('float32'))\n\n    )\n\n  ),\n\n  key=LinearGeneral( # Param: 66 (264 B)\n\n    in_features=(10,),\n\n    out_features=(2, 3),\n\n    axis=(-1,),\n\n    batch_axis=FrozenDict({}),\n\n    use_bias=True,\n\n    dtype=None,\n\n    param_dtype=float32,\n\n    kernel_init=&lt;function variance_scaling.&lt;locals&gt;.init at 0x73e5c28e2700&gt;,\n\n    bias_init=&lt;function zeros at 0x73e5c362c5e0&gt;,\n\n    precision=None,\n\n    dot_general=None,\n\n    dot_general_cls=None,\n\n    promote_dtype=&lt;function promote_dtype at 0x73e5c28e2160&gt;,\n\n    kernel=Param( # 60 (240 B)\n\n      value=Array(shape=(10, 2, 3), dtype=dtype('float32'))\n\n    ),\n\n    bias=Param( # 6 (24 B)\n\n      value=Array(shape=(2, 3), dtype=dtype('float32'))\n\n    )\n\n  ),\n\n  value=LinearGeneral( # Param: 66 (264 B)\n\n    in_features=(10,),\n\n    out_features=(2, 3),\n\n    axis=(-1,),\n\n    batch_axis=FrozenDict({}),\n\n    use_bias=True,\n\n    dtype=None,\n\n    param_dtype=float32,\n\n    kernel_init=&lt;function variance_scaling.&lt;locals&gt;.init at 0x73e5c28e2700&gt;,\n\n    bias_init=&lt;function zeros at 0x73e5c362c5e0&gt;,\n\n    precision=None,\n\n    dot_general=None,\n\n    dot_general_cls=None,\n\n    promote_dtype=&lt;function promote_dtype at 0x73e5c28e2160&gt;,\n\n    kernel=Param( # 60 (240 B)\n\n      value=Array(shape=(10, 2, 3), dtype=dtype('float32'))\n\n    ),\n\n    bias=Param( # 6 (24 B)\n\n      value=Array(shape=(2, 3), dtype=dtype('float32'))\n\n    )\n\n  ),\n\n  query_ln=None,\n\n  key_ln=None,\n\n  out=LinearGeneral( # Param: 84 (336 B)\n\n    in_features=(2, 3),\n\n    out_features=(12,),\n\n    axis=(-2, -1),\n\n    batch_axis=FrozenDict({}),\n\n    use_bias=True,\n\n    dtype=None,\n\n    param_dtype=float32,\n\n    kernel_init=&lt;function variance_scaling.&lt;locals&gt;.init at 0x73e5c28e2700&gt;,\n\n    bias_init=&lt;function zeros at 0x73e5c362c5e0&gt;,\n\n    precision=None,\n\n    dot_general=None,\n\n    dot_general_cls=None,\n\n    promote_dtype=&lt;function promote_dtype at 0x73e5c28e2160&gt;,\n\n    kernel=Param( # 72 (288 B)\n\n      value=Array(shape=(2, 3, 12), dtype=dtype('float32'))\n\n    ),\n\n    bias=Param( # 12 (48 B)\n\n      value=Array(shape=(12,), dtype=dtype('float32'))\n\n    )\n\n  ),\n\n  rngs=None,\n\n  cached_key=None,\n\n  cached_value=None,\n\n  cache_index=None\n\n)"
  },
  {
    "objectID": "posts/rambling001/index.html",
    "href": "posts/rambling001/index.html",
    "title": "Notes on neural scaling and its value to biology",
    "section": "",
    "text": "I saw a plot recently, showing the size of scRNA-seq datasets according to release date. Tahoe-100M includes 100M cells and at least looking at this plot, it looks like we have had an exponential increase in the size of the of the largest data sets.\n\n\n\nTahoe-100M\n\n\nSimilar growth in dataset sizes have happened in genomics and elsewhere.\nA clear question that arises in this context is then: Can we leverage the growth in data and convert it to value?\nFor LLMs, the story has been that as compute costs have decreased, we (they) have been able to train on larger datasets, but the main point is that pre-training has allowed for actually useful products like chatbots.\nAs pre-training has scaled there might have been diminishing returns, and statements by OpenAI regarding GPT-4.5 being the last in the series of large models can be seen as an indication that there has been a limit to pre-training, which has then been taken over by another regime of scaling RL. In any of these cases of scaling, the core is that we have found a way to make the investment in compute worth it, found a way to actually harness the data. Relevant to this question is neural scaling laws\n\nNeural Scaling laws\n\n\n\nChinchilla\n\n\nModern machine learning has entered a paradigm, where scaling laws play an important role both in practice and at the theoretical level. In the regime where we are not bottlenecked by data (though scaling laws do not need this), it has been observed that there are relationships between data size \\(D\\), number of parameters in model \\(P\\), and floating points operations \\(C\\) (compute), that are quite clean. Scaling laws are not restricted to these quantities, and people are actively investigating other relationships and quantities such as batch size and learning rate. One example of an observed scaling law is in the area of compute optimal training \\[\nL_C = E + c_D D^{-\\alpha_D} + c_P P^{-\\alpha_P}\n\\]\nConsider the case where we are training a generative model \\(q(x)\\) to minimize cross-entropy \\(H(p_{data},q)\\) (maximise likelihood of model under data distribution). The cross-entropy can be divided into entropy of the data distribution and the KL-divergence of data distribution to model \\[\nH(p_{data},q) = H(p_{data}) + KL(p_{data}||q)\n\\] Scaling laws these are based on fitting parametric function to the data we have about training runs of different sizes etc, so there are limits to how seriously we should take a specific formula for scaling laws, but in this picture \\(E\\) corresponds to the inherent entropy of the data distribution, the noise level.\nLooking at the scaling law, in this picture the KL divergence which is always non-negative is being driven to zero as we increase D and N. How fast depends on constants \\(\\alpha_D\\) and \\(\\alpha_P\\). The image is now one where you decide how many FLOPs you are willing to use to train your model, and then determine the optimal number of parameters of your model and how much data you are going to train on.\nAs has been clear for a long time, this of course ignores the fact that reducing the number of flops needed to reach your desired level of loss is a small part of the picture. After training when you want to make use of the models, bigger models are unwieldy, they require more expensive hardware accelerators to run them on, and ofc they require more flops to run, have higher latency (time to first token generated) and throughput (number of tokens generated per second) than smaller models.\nJust as importantly, the loss you reach in pre-training does not directly tell you how well you will perform on a downstream task you care about. After you have pre-trained your model, you want to post-train it, tuning it to do the thing you care about, like being a good chatbot or agent. Only once you have signs that lower loss is robustly correlated with better performance can you justify acquiring the flops and data needed to get a pre-trained model with the low loss that you want.\n\n\nThe value of performance\nIn the context of LLMs, pre-training data is relatively abundant, even if there is a lot of work to do about ensuring quality of data. In the domain of post-training the abundance of data depends on the task, you might be limited in terms of quality preference data (which response from the chatbot is better, answer A or B?), but in some cases such as software and computer-use agents, given sufficient work, there is the possibility of generating abundant synthetic data, that is directly valuable, by letting agents interact with an environment in a way that closely mimicks the final use case.\nIt seems that in biology, the picture becomes a bit more muddy. The performance of an LLM on a suite of benchmark can give at least a partial picture of the overall quality, and especially for benchmarks which aim to closely mimick a final use case, it gets close to being a description of the economic value of using the model.\nComputational biology methods often are diverse, some methods can give you certain information that can then be used in a variety of ways, and at some point downstream of all these methods being applied, and applied the right way, we might simplistically view it as there being derived some scientific understanding of value or changes of health care outcomes of value.\nI am trying to get at some kind of picture where given a certain model with a certain performance on a benchmark, such a model has a certain value for final use cases.\nThere is an observed market, where chatbot providers provide subscriptions and user’s are paying for access to a model with a certain performance. We are not in a situation where model providers can smoothly trade-off inference cost with performance but this is not unimaginable at all. We could use this to make a simplistic picture of the situation. Assume we simplify performance to a scalar value \\(E\\) even though it we cannot in practice, then we can now imagine for a fixed performance \\(E\\) there exist demand and supply curves with equilibrium \\(P_*^E\\) where \\(Q^E_d(P) = Q^E_s(P)\\). We are now having a collection of such markets indexed by performance, and a function \\(P_*(E)\\), being one way of describing the value of performance.\nSuch a thing is hard to imagine for biology."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "kasper munk rasmussen",
    "section": "",
    "text": "Notes on neural scaling and its value to biology\n\n\n\nstream-of-thought\n\n\n\n\n\n\n\n\n\nJun 2, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nHow does static FC compare across sessions in MyConnectome Project?\n\n\n\nneuroscience\n\nfmri\n\n\n\n\n\n\n\n\n\nJun 2, 2025\n\n\n\n\n\n\n\n\n\n\n\n\njax adventures 001\n\n\n\n\n\n\n\n\nJun 2, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/srfcmyconnectome/index.html",
    "href": "posts/srfcmyconnectome/index.html",
    "title": "How does static FC compare across sessions in MyConnectome Project?",
    "section": "",
    "text": "One could imagine that one day it might be possible that a psychiatric patient could be undergoing some brain imaging sessions, and some computational model of the subject’s brain will be used for guiding treatment.\nMyConnectome Project has a lot of brain imaging recording from the same person. Perhaps it could be used as a playground to ask some questions about what possibilities and limitations there may be for this idea of making a personalized brain model for a patient.\n\nFormal notations for fMRI\nA common object in computational analyses of fMRI is the functional connectivity matrix. Very simplified, fMRI gives you at each timepoint a 3d matrix \\(\\omega_t \\in \\mathbb{R}^{W \\times H \\times D}\\), describing “activity” at each coordinate. Therefore this \\(v_t\\) is a quite high-dimensional object and is reduced by aggregating activity in different “brain areas”.\nWe can think of \\(\\Omega = [W] \\times [H] \\times [D]\\) as the domain on which the original signal lives, where \\([W] = \\{0,...,W-1\\}\\).\nNow each \\(v_t\\), is a mapping \\(v_t : \\Omega \\to \\mathbb{R}\\), from each point in the domain to a value of activity.\nThis domain \\(\\Omega\\) obviously has some kind of structure, in some cases associate voxels with their Euclidean coordinates. This makes some sense, since we are considering \\(\\Omega\\) as the space of immediate signals. We might be trying to infer something about what the structure of the underlying brain state leading to a volume or a time series of volumes has been, but this brain state would then live in another space than the space of recorded volumes.\nAn “atlas” or “parcellation” is a partition of \\(\\Omega\\), a set of disjoint subsets. In some cases we might not require that the union of the sets constite \\(\\Omega\\), because what if some coordinate is outside the brain? We get a volume that is a box but the brain is not a box. We write \\(A_1, ..., A_N \\subset \\Omega\\) of disjoint subsets \\(A_n\\) of the domain. N will often be in the order of 100.\nNow we can consider looking at a specific area, and consider only the parts of the signal in that area. \\(v_t^m : A_n \\to \\mathbb{R}\\). We can also consider the some method of aggregation: If an area \\(A_n\\) has cardinality \\(|A_n|\\), then we can consider a the set \\(\\{v_t^n(\\omega) \\in \\mathbb{R} | \\forall \\omega \\in A_n \\} \\subseteq \\mathbb{R}\\), the set of all activities in that brain region at that time. One way of aggregating is to just take the mean, call it \\(\\mu^n_t \\in \\mathbb{R}\\).\nGiven some selection of an “atlas”, we say that we parcellate a recording \\(\\{v_t\\}_{t\\in[T]}\\) by converting it to parcellated time series \\(\\{\\mu_t^n\\}_{t\\in [T],n\\in[N]}\\).\nThe parcellated time series constitute a reduced representation of the recording, in practice it will be a matrix \\(M \\in \\mathbb{R}^{T\\times N}\\).\nOne hopes that this object \\(M\\) captures something meaningful about the brain recording, by averaging over the smaller activities, that the noise is thereby reduced.\nOne processing step that might be applied is to filter the frequencies. Since the our recording \\(v_t\\) is made at discrete timesteps \\(t\\), that are made every TR (repetition time) seconds in the real world, where TR is then the sampling interval. This and other details, make it so that there are principled reasons for doing frequency filtering of the time series. We will assume that \\(M\\) is consisting of processed time series.\nThe basis of functional connectivity is to consider correlations between pairs \\(\\mu_t^n\\), \\(\\mu_t^m\\), for \\(m,n \\in [N]\\), using standard Pearson correlation. That is we consider them as vectors in \\(\\mathbb{R}^T\\).\nFor two vectors \\(x, y \\in \\mathbb{R}^T\\), the Pearson correlation is the cosine distance between mean-centered versions. For two random variables \\(X\\) and \\(Y\\), the correlation is \\[\n\\text{Corr}(X,Y) = \\frac{Cov(X,Y)}{\\sqrt{Var(X)}\\sqrt{Var(Y)}}\n\\]\nand Pearson’s rho is an"
  }
]