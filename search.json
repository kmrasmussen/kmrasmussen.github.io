[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "posts",
    "section": "",
    "text": "The Long Tail: Thoughts on Data Collection and Ownership in the Interpolation Revolution\n\n\n\nai\n\nsociety\n\nllm\n\n\n\n\n\n\n\n\n\nJul 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nRole of Open Source in AI-Human Collaboration\n\n\n\n\n\n\n\n\nJun 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nmacromacroscale\n\n\n\n\n\n\n\n\nJun 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nmeditation attractors\n\n\n\nmeditation\n\n\n\n\n\n\n\n\n\nJun 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\ncurious adventures 1\n\n\n\n\n\n\n\n\nJun 3, 2025\n\n\n\n\n\n\n\n\n\n\n\n\njax adventures 001\n\n\n\n\n\n\n\n\nJun 2, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nHow does static FC compare across sessions in MyConnectome Project?\n\n\n\nneuroscience\n\nfmri\n\n\n\n\n\n\n\n\n\nJun 2, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nNotes on neural scaling and its value to biology\n\n\n\nstream-of-thought\n\n\n\n\n\n\n\n\n\nJun 2, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nShould LLMs have the ability to stop a conversation?\n\n\n\nllm\n\nconsciousness\n\n\n\n\n\n\n\n\n\nDec 29, 2024\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "kasper munk rasmussen",
    "section": "",
    "text": "My name is Kasper. My interests include machine learning, LLMs, computational biology and computational neuroscience and AI safety. I did a Master’s degree in Computer Science from ETH Zürich, focusing mostly on machine learning, and subsequently I have worked as a full-stack software developer at a large Danish software consultancy on a Danish government project. I am currently looking for a job that stokes my passion. Feel free to contact me.\nBlog - Though I have a very disorganized and non-polished blog, you might get some sense of my interests. Feel free to reach out if you’d like to discuss any topic."
  },
  {
    "objectID": "index.html#selected-projects",
    "href": "index.html#selected-projects",
    "title": "kasper munk rasmussen",
    "section": "Selected Projects",
    "text": "Selected Projects\n\nintercebd.com - With intercebd.com you can collect and annotate and propose LLM responses. You can create SFT and DPO fine-tuning datasets, and use them with OpenAI’s fine-tuning service or push the datasets to Huggingface in ChatML format. Built with a FastAPI/Postgres backend and OpenRouter. GitHub\nCS MSc Thesis: Understanding Features in Superposition in Transformer Language Models (PDF): In 2023, concurrently with Anthropic’s first SAEs trained on larger LLMs, I formalized notions of superposition that allowed for falsifiable experiments, and showed restricted kinds of superposition in realistic pre-trained Transformer LLMs using linear probes and causal interventions. I was supervised by Mor Geva."
  },
  {
    "objectID": "index.html#videos",
    "href": "index.html#videos",
    "title": "kasper munk rasmussen",
    "section": "Videos",
    "text": "Videos\nI am exploring streaming my screen, talking about machine learning and programming.\n\n\n\n\n\n\n\n\n\n\nHow a Transformer LLM works - Part 1 Residual stream and predictive distribution\n\n\n\nml\n\nllm\n\n\n\n\nJun 17, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nBasic Math of Supervised Deep Learning with Cross-Entropy\n\n\n\nml\n\n\n\n\nJun 16, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nML Paper Walkthrough: CodeLutra Boosting LLM Code Generation via Preference-guided Refinement\n\n\n\nml\n\nllm\n\n\n\n\nSep 14, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#recent-posts",
    "href": "index.html#recent-posts",
    "title": "kasper munk rasmussen",
    "section": "Recent posts",
    "text": "Recent posts\nSee all posts →\n\n\n\n\n\n\n\n\n\n\nThe Long Tail: Thoughts on Data Collection and Ownership in the Interpolation Revolution\n\n\n\n\n\n\n\n\nJul 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nRole of Open Source in AI-Human Collaboration\n\n\n\n\n\n\n\n\nJun 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nmacromacroscale\n\n\n\n\n\n\n\n\nJun 10, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "kasper munk rasmussen",
    "section": "Education",
    "text": "Education\n\nMaster of Computer Science, ETH Zürich\n09/2021 - 11/2023\n\nMajor in Machine Intelligence, minor in Theoretical Computer Science\nElectives focused on neuroscience and bioinformatics\nThesis (top grade): Understanding Features in Superposition in Transformer Language Models\nGPA: 5.4/6\n\n\n\nBachelor of Computer Science, University of Copenhagen\n09/2018 - 06/2021\n\nData Science specialization\nThesis (top grade): Unsupervised learning of objects and concepts with a focus on medical images\nGPA: 11.6/12"
  },
  {
    "objectID": "index.html#work-experience",
    "href": "index.html#work-experience",
    "title": "kasper munk rasmussen",
    "section": "Work experience",
    "text": "Work experience\n\nAI consultant, Tiimo\n09/2024 -\n\nHelped Tiimo use LLM in new features\nOpenAI API, structured generation, evals, FastAPI, dockerization\n\n\n\nSoftware Developer, Netcompany\n03/2024 - 02/2025\n\nBorger.dk, the most visited Danish government website.\nFull-stack, C#, Sitecore, HTML/JS, Azure DevOps, Git, agile"
  },
  {
    "objectID": "posts/rambling001/index.html",
    "href": "posts/rambling001/index.html",
    "title": "Notes on neural scaling and its value to biology",
    "section": "",
    "text": "I saw a plot recently, showing the size of scRNA-seq datasets according to release date. Tahoe-100M includes 100M cells and at least looking at this plot, it looks like we have had an exponential increase in the size of the of the largest data sets.\n\n\n\nTahoe-100M\n\n\nSimilar growth in dataset sizes have happened in genomics and elsewhere.\nA clear question that arises in this context is then: Can we leverage the growth in data and convert it to value?\nFor LLMs, the story has been that as compute costs have decreased, we (they) have been able to train on larger datasets, but the main point is that pre-training has allowed for actually useful products like chatbots.\nAs pre-training has scaled there might have been diminishing returns, and statements by OpenAI regarding GPT-4.5 being the last in the series of large models can be seen as an indication that there has been a limit to pre-training, which has then been taken over by another regime of scaling RL. In any of these cases of scaling, the core is that we have found a way to make the investment in compute worth it, found a way to actually harness the data. Relevant to this question is neural scaling laws\n\nNeural Scaling laws\n\n\n\nChinchilla\n\n\nModern machine learning has entered a paradigm, where scaling laws play an important role both in practice and at the theoretical level. In the regime where we are not bottlenecked by data (though scaling laws do not need this), it has been observed that there are relationships between data size \\(D\\), number of parameters in model \\(P\\), and floating points operations \\(C\\) (compute), that are quite clean. Scaling laws are not restricted to these quantities, and people are actively investigating other relationships and quantities such as batch size and learning rate. One example of an observed scaling law is in the area of compute optimal training \\[\nL_C = E + c_D D^{-\\alpha_D} + c_P P^{-\\alpha_P}\n\\]\nConsider the case where we are training a generative model \\(q(x)\\) to minimize cross-entropy \\(H(p_{data},q)\\) (maximise likelihood of model under data distribution). The cross-entropy can be divided into entropy of the data distribution and the KL-divergence of data distribution to model \\[\nH(p_{data},q) = H(p_{data}) + KL(p_{data}||q)\n\\] Scaling laws these are based on fitting parametric function to the data we have about training runs of different sizes etc, so there are limits to how seriously we should take a specific formula for scaling laws, but in this picture \\(E\\) corresponds to the inherent entropy of the data distribution, the noise level.\nLooking at the scaling law, in this picture the KL divergence which is always non-negative is being driven to zero as we increase D and N. How fast depends on constants \\(\\alpha_D\\) and \\(\\alpha_P\\). The image is now one where you decide how many FLOPs you are willing to use to train your model, and then determine the optimal number of parameters of your model and how much data you are going to train on.\nAs has been clear for a long time, this of course ignores the fact that reducing the number of flops needed to reach your desired level of loss is a small part of the picture. After training when you want to make use of the models, bigger models are unwieldy, they require more expensive hardware accelerators to run them on, and ofc they require more flops to run, have higher latency (time to first token generated) and throughput (number of tokens generated per second) than smaller models.\nJust as importantly, the loss you reach in pre-training does not directly tell you how well you will perform on a downstream task you care about. After you have pre-trained your model, you want to post-train it, tuning it to do the thing you care about, like being a good chatbot or agent. Only once you have signs that lower loss is robustly correlated with better performance can you justify acquiring the flops and data needed to get a pre-trained model with the low loss that you want.\n\n\nThe value of performance\nIn the context of LLMs, pre-training data is relatively abundant, even if there is a lot of work to do about ensuring quality of data. In the domain of post-training the abundance of data depends on the task, you might be limited in terms of quality preference data (which response from the chatbot is better, answer A or B?), but in some cases such as software and computer-use agents, given sufficient work, there is the possibility of generating abundant synthetic data, that is directly valuable, by letting agents interact with an environment in a way that closely mimicks the final use case.\nIt seems that in biology, the picture becomes a bit more muddy. The performance of an LLM on a suite of benchmark can give at least a partial picture of the overall quality, and especially for benchmarks which aim to closely mimick a final use case, it gets close to being a description of the economic value of using the model.\nComputational biology methods often are diverse, some methods can give you certain information that can then be used in a variety of ways, and at some point downstream of all these methods being applied, and applied the right way, we might simplistically view it as there being derived some scientific understanding of value or changes of health care outcomes of value.\nI am trying to get at some kind of picture where given a certain model with a certain performance on a benchmark, such a model has a certain value for final use cases.\nThere is an observed market, where chatbot providers provide subscriptions and user’s are paying for access to a model with a certain performance. We are not in a situation where model providers can smoothly trade-off inference cost with performance but this is not unimaginable at all. We could use this to make a simplistic picture of the situation. Assume we simplify performance to a scalar value \\(E\\) even though it we cannot in practice, then we can now imagine for a fixed performance \\(E\\) there exist demand and supply curves with equilibrium \\(P_*^E\\) where \\(Q^E_d(P) = Q^E_s(P)\\). We are now having a collection of such markets indexed by performance, and a function \\(P_*(E)\\), being one way of describing the value of performance.\nSuch a thing is hard to imagine for biology."
  },
  {
    "objectID": "posts/srfcmyconnectome/index.html",
    "href": "posts/srfcmyconnectome/index.html",
    "title": "How does static FC compare across sessions in MyConnectome Project?",
    "section": "",
    "text": "One could imagine that one day it might be possible that a psychiatric patient could be undergoing some brain imaging sessions, and some computational model of the subject’s brain will be used for guiding treatment.\nMyConnectome Project has a lot of brain imaging recording from the same person. Perhaps it could be used as a playground to ask some questions about what possibilities and limitations there may be for this idea of making a personalized brain model for a patient.\n\nFormal notations for fMRI\nA common object in computational analyses of fMRI is the functional connectivity matrix. Very simplified, fMRI gives you at each timepoint a 3d matrix \\(\\omega_t \\in \\mathbb{R}^{W \\times H \\times D}\\), describing “activity” at each coordinate. Therefore this \\(v_t\\) is a quite high-dimensional object and is reduced by aggregating activity in different “brain areas”.\nWe can think of \\(\\Omega = [W] \\times [H] \\times [D]\\) as the domain on which the original signal lives, where \\([W] = \\{0,...,W-1\\}\\).\nNow each \\(v_t\\), is a mapping \\(v_t : \\Omega \\to \\mathbb{R}\\), from each point in the domain to a value of activity.\nThis domain \\(\\Omega\\) obviously has some kind of structure, in some cases associate voxels with their Euclidean coordinates. This makes some sense, since we are considering \\(\\Omega\\) as the space of immediate signals. We might be trying to infer something about what the structure of the underlying brain state leading to a volume or a time series of volumes has been, but this brain state would then live in another space than the space of recorded volumes.\nAn “atlas” or “parcellation” is a partition of \\(\\Omega\\), a set of disjoint subsets. In some cases we might not require that the union of the sets constite \\(\\Omega\\), because what if some coordinate is outside the brain? We get a volume that is a box but the brain is not a box. We write \\(A_1, ..., A_N \\subset \\Omega\\) of disjoint subsets \\(A_n\\) of the domain. N will often be in the order of 100.\nNow we can consider looking at a specific area, and consider only the parts of the signal in that area. \\(v_t^m : A_n \\to \\mathbb{R}\\). We can also consider the some method of aggregation: If an area \\(A_n\\) has cardinality \\(|A_n|\\), then we can consider a the set \\(\\{v_t^n(\\omega) \\in \\mathbb{R} | \\forall \\omega \\in A_n \\} \\subseteq \\mathbb{R}\\), the set of all activities in that brain region at that time. One way of aggregating is to just take the mean, call it \\(\\mu^n_t \\in \\mathbb{R}\\).\nGiven some selection of an “atlas”, we say that we parcellate a recording \\(\\{v_t\\}_{t\\in[T]}\\) by converting it to parcellated time series \\(\\{\\mu_t^n\\}_{t\\in [T],n\\in[N]}\\).\nThe parcellated time series constitute a reduced representation of the recording, in practice it will be a matrix \\(M \\in \\mathbb{R}^{T\\times N}\\).\nOne hopes that this object \\(M\\) captures something meaningful about the brain recording, by averaging over the smaller activities, that the noise is thereby reduced.\nOne processing step that might be applied is to filter the frequencies. Since the our recording \\(v_t\\) is made at discrete timesteps \\(t\\), that are made every TR (repetition time) seconds in the real world, where TR is then the sampling interval. This and other details, make it so that there are principled reasons for doing frequency filtering of the time series. We will assume that \\(M\\) is consisting of processed time series.\nThe basis of functional connectivity is to consider correlations between pairs \\(\\mu_t^n\\), \\(\\mu_t^m\\), for \\(m,n \\in [N]\\), using standard Pearson correlation. That is we consider them as vectors in \\(\\mathbb{R}^T\\).\nFor two vectors \\(x, y \\in \\mathbb{R}^T\\), the Pearson correlation is the cosine distance between mean-centered versions. For two random variables \\(X\\) and \\(Y\\), the correlation is \\[\n\\text{Corr}(X,Y) = \\frac{Cov(X,Y)}{\\sqrt{Var(X)}\\sqrt{Var(Y)}}\n\\]\nand Pearson’s rho is an"
  },
  {
    "objectID": "posts/curiousadventures1/index.html",
    "href": "posts/curiousadventures1/index.html",
    "title": "curious adventures 1",
    "section": "",
    "text": "brain things\nwhat is the first thing i want to know? there is on the order of 100b neurons, people throw around the number 86 billion. okay, but there are not only neurons. how many non-neurons are there? okay, interesting, 4o says also O(100b) glial cells.\nwhat are glial cells? okay it looks like at least in the brain, i need mainly to know 3 types of glial cells:\n\n50% oligodendrocytes - a type of glial cell that wraps around the axon forming myelin, speeding up electric signaling.\n30% astro - maintain extracellular matrix, blood flow\n10% microglia - immune cells, pruning synapses, remove debris\n\nso i guess astrocytes are the most important.\n\nthey clear excess neurotransmitters like glutamate, so there is not too much which would result in overactivation of the brain, so that seems quite important for even computation.\nwhen a brain region becomes active the astrocytes sense changes in neurotransmitters and signal to blood vessels to dilate, crazy. called neurovascular coupling, important for understanding fMRI.\nrelease signals that influence synapses, so strength, learning, plasticity\ncan become “reactive astrocytes” in infection, alzheimers etc.\n\nokay that is enough for me for now about astrocytes. and the oligodendrocytes kind of makes sense to me not going deeper with that.\nthere is 1e8 neurons and 1e8 glial cells. but like oligodendrocytes are forming myelin always or also different things, and in that case is one oligodendrocyte only responsible for one axon, and how many axons? okay oligodendrocytes myelinate multiple axons. 1 axon per neuron but they can branch. estimated that 50% of the axons in the brain are myelinated but depends on regions. okay the primary difference is gray and white matter: 90% of white, and maybe 20% of gray.\nbut i did not learn what white and gray is. okay, it sounds a bit bullshitish, not easy enough to get a clear answer. roughly\n\ngray matter: high number of somas per volume\nwhite matter: low number of somas per volume AND high number of myelinated axons per volume\n\nat least i will run with this. so now i am getting a picture of the brain as having some kind of volume in R^3 and there is some kind of neuron soma count measure that describes how many somas are in a specific region, and then abstracting it as a density even though not continous giving neuron soma density. And then there is an axon count measure and axon count density and myelinated axon count measure. And these are the interesting things, and gray and white matter are simplifications.\n(okay, just got distracted because presented 4o that i wanted to think in terms of measures and it wanted to go off with fiber bundles. so learned that the tangent bundle is the a disjoiint union of the tangent spaces for each point on the manifold, disjoint in the sense that you keep track of which point it came from. so disjoint union is a bit like enumerate in python, you keep an index.)\nokay so where were we. we have neurons with somas and axons and glial cells especially astrocytes and oligodendrocytes. so we are still looking at it from afar. something about energy. how much energy is it using and are neurons using more energy than glial cells?\n\nthe energy usage is on the order of 20 W and roughly 20% of the brain’s energy.\nneurons use 75% and glial cells then less than 20%.\n\nthe reason why they use relatively much has to do with action transmission (ion gradients, glutamate release/reuptake)\nokay so one thing is the density of neurons and the density of glial cells and their relationships. we saw that the notion of gray matter means that neuron soma density is not uniform, is the astrocytes the same way. ok it sounds like i should think of astrocyte density being related to neuron density, because neurons “need” astrocytes, so more neurons need more astrocytes. so astrocyte denisty is not uniform. locally it can be viewed a bit as a regular lattice.\nokay i wonder how one can know these kind of things. spatial transcriptomics of neuronal cell cultures? some methods can do single cells RNA others do aggregate per “spot”? Okay it sounds like you take a mouse brain flash freeze it make a slice for example from the cortex (barbaric unless it already was dying) of 10 \\(\\mu m\\) (mu means micro so one 1 \\(\\mu m\\) is 1e-6m).\n\nThe soma of a neuron is on the order of 10 micrometers, 1e-5 meters.\nsame for astrocytes 10 micrometers, but that is the cell body, it has “processes extending” that are on the order of 100 micrometers which would be its radius.\n\nhmm, i am starting to think about how i should imagine it. if i have a cube with sidelength 100 micrometers, giving what proportion of the volume is taken up by different things? 4o says\n\n10% neuron somas\n10* glial somas\n45% glial and neuron processes (dendrites axons etc)\n20% extracellular space with water and ions and neurotransmitters etc\n5% capillaries\n\nand they form some kind of soft tissue.\nok that was quite a high degree of “exploitation” in this line, a lot of focus on astrocytes etc. i will continue a bit\none of the things we learned was 75% of energy going to neurons because of spiking. i can imagine energy requirements changing over time, after all that is what fmri is about, but are there some regions that are just more energy intesive, and is that mostly predicted by neuron soma density?\nfirst, like how can we measure? FDG is a glucose that is modified to emit positrons and can then image with PET-scan. Cortex, especially frontal and visual cortex, uses a lot of glucose.\n\n\ninside cells\nso we have these cels they are maybe 10 micrometers? what’s going on inside there. there is a nucleus, i guess it is like a ball. it is roughly 5 micrometers diameter, it takes up 10% of the volume. does that match? yeah it is okay. and the mitrochondria also 10% only one nucleus and multiple mitochrondira?\n\norder of 100 to 1000 mitochrondria per cell\nshaped like 0.5x2 micrometers\n\ngreat and inside the nucleus, the chromatin fills up quite well even though not taking up all the volume\n\norder of 10-100 chromosomes, 46 for humans\n3e9 basepairs total, varying from 50 to 250 million base pairs per chromosome, decreasing pretty linearly with rank\n\n\n\n\nChromosome lengths\n\n\n\nokay and it is like this: 8 histone proteins form as 2x2x2 box and order of 100 base pairs wrap around it\non one side is a H1 histone where the dna enters and exits\norder of 10-100 base pairs of linker dna to next structure\nthe structures are called nucleosomes\n\n\n\n\nNucleosome cartoon\n\n\n\nthe word chromatin somehow refers to all of these nucleosomes and some other proteins etc not super clear\nword chromosome also being used a bit weird, focused on when the cell is diving etc, but also numbering them. so each chromosome there is “chromatin” for that chromosome somewhere in the cell, seems you can say “the chromatin for chromosome 11”\nthere are 3 RNA-polymerases, it is number 2 that is doing all the the transcription for real proteins.\ncan transcribe sections that are longer than is on a single nucleosome so there is a crazy dance with other protein complexes like FACT etc that unfold the nucleosome etc.\nmultiple different RNA-polymerase-2’s can be transcribing at the same time at the same interval\nspeed is O(1K) basepairs per minute, can vary by factor 3x depending\nO(50K) RNA-polymerase-2’s inside the nucleus at a single time. O(15K) transcribing at a given time.\nO(1e6) transcripts in the cell at given time, O(1e10) proteins in the cell at given time, one transcript can be translated to protein multiple times\n30% of the volume in cytoplasma taken up by macromoleucles like RNA and protein. 50% is protein, 25% RNA. I am confused concluded that proteins were much more abundant? it’s because RNA is much larger, nucleotides are bigger than amino acids. think 4o might be starting to hallucinate now that i am inquiring about this.\nokay amino acids are O(50) Å^3 (1Å = 0.1 nanometer, 1e-10 meters) and nucleotides O(500) so one order of magnitude difference, but we had 3 ooms difference in abundance of rna and protein.\nokay seems it is because of rRNA in ribosomes very huge, and makes up 80% of RNA in the cell\nthe ribosome is made up with a core part of it being rRNA that is why it is huge.\ntranslation rate is O(10) AA/s, 30-60 seconds per protein typical\nO(1e6) ribosomes in the cell, ~20 nm in diameter, ~7K nucleotides for the rRNA."
  },
  {
    "objectID": "posts/thoughtadventures1/index.html",
    "href": "posts/thoughtadventures1/index.html",
    "title": "meditation attractors",
    "section": "",
    "text": "when doing breath meditation, mindwandering is an important notion. we can perhaps imagine that the mind follows a trajectory in some kind of space, \\(x(t) \\in \\mathcal{X}\\). We can also imagine that maybe this trajectory is stochastic in some sense, but maybe will wait.\nI will use it as a word “introspectively” for “what i conclude when i try to think about what i experience in meditation”.\nIntrospectively, time is very fine-grained an consists of a single percept. So in that case we have to think of \\(\\mathcal{X}\\) as a set of percpets. These percepts are complex objects, and one important aspect is that they differ by many “ooms” in how much they cause some kind of reaction. This means that some percept, it can be an internal image of a person, can have a certain kind of salience. Introspectively, percepts with a high value are “on average” followed by more of percepts that are “related”.\nSimplistically we might imagine some kind of entity that controls inputs from multiple different sources. These sources may be more “sensory” or they might be more “internal”, such as images generated by fantasies or planning etc.\nI then imagine \\(x(t)\\) as a kind of controlled sampling from these sources, where each source has its own trajectories and you get a sample from one of them.\nThis controlled sampler is a mechanism of attention. I then imagine that we can view the space from two perspectives, we can view it as discrete moments where we get a sample from each, or we can look at the relative abundance of samples from each source, almost as if we are looking across small intervals \\(\\Delta t\\).\nI don’t know whether to think of these sources as specific brain areas or more distributed, for now we are restricting to a kind of combination of introspection and formalizing, where we try to throw around ideas for formalizations of what we introspect. Introspection is different from awareness of the things, it invovles a kind of balance between rationalization and awareness.\nHave you ever seen these videos showing a world map and the change of borders as different countries conquer each other an so on? There is some kind of finite resource, the area of land in the world and then there are these entities, countries, that vary in their territory. New ones can arise at some point and also end at some point."
  },
  {
    "objectID": "posts/thoughtadventures1/index.html#criticality-in-statistical-mechanics",
    "href": "posts/thoughtadventures1/index.html#criticality-in-statistical-mechanics",
    "title": "meditation attractors",
    "section": "criticality in statistical mechanics",
    "text": "criticality in statistical mechanics\nIn statistical mechanics, scale free is related to criticality. Consider a (2d) grid/lattice of points, that can take on values -1 or +1. A specific configuration of -1 or 1 is the state of the system. We are not concerned about how it evolves, but about the distribution. A Boltzman distribution is a distribution that can be written \\(p(x) \\sim \\exp(-\\beta H(x))\\). If we have a function \\(H(x)\\) (a Hamiltonian) that assigns a scalar value (“energy”) to a specific configuration \\(x\\) of the grid, then we can get a distribution over configurations. This distribution depends on how we choose our H function, but once that is fixed it also depends on \\(\\beta\\). In statmech foundations of thermodynamics \\(\\beta\\) is literally associated with the literal temperature of the system \\(\\beta \\propto 1/T\\), but we can consider it more general. The picture is rather that we have some parameters like \\(\\beta\\) and they determine the distribution on the random field (eg the grid), but it is easier if we think of a continuous field rather than a grid. So this is just an explanation of what it could mean that we have a random field, with a specific correlation function, and we can imagine that there are ways in which this random field is translation invariant and rotation invaration so that we only care about distances of points, so we have \\(G(r)\\) for the correlation. As we said, there might be a parameter such as \\(\\beta\\) that influences the distribution and therefore the random field, and therefore it can also influence the correlation function. In particular we can imagine the correlation decays with distance r, but how fast depends on the parameter such as temperature \\(T\\) \\[\nG(r; T) \\sim \\exp(-r/\\xi(T))\n\\] this \\(\\xi(T)\\) is called the correlation length. Statistical mechanics is used to describe phase transitions in systems, where systems changes from behaving in one way to a “qualitatively” different way. We often hear the example of ice and water, the same things but different temperature, but this stuff with criticality is used to describe “second order phase transitions”, and water to ice is not that so not all phase transitions are like that. But such a phase transition happens at some “temperature” or other value of some parameter, and the relation is that it is a this special temperature we call it \\(T_c\\) that the correlation is instead a power law \\[\nG(r;T) \\sim r^{-\\alpha}\n\\] this means that there are “stronger” correlations between distant points, not in the dynamic sense but under the boltzman distribution.\nFor the critical point, as \\(T\\) comes closer to \\(T_c\\) the correlation length becomes longer and longer, and it diverges \\[\n\\lim_{T \\to T_c} \\xi(T) = \\infty\n\\] but also, and i am not sure about the exact relatinship, it is modelled close to T as \\[\n\\xi(T) \\sim |T - T_c|^{-\\nu}\n\\] where \\(\\nu\\) is the correlation length exponent, and you can see how this means that it become larger and larger as we get closer.\nWith the scaling hypothesis the power law decay and exponential decays is bridged somehow with a function \\(f(u) = \\exp(-u)\\) that becomes constant near the critical point \\(T_c\\) so that the correlation function can be written in general as \\[\nG(r; T) \\sim \\frac{1}{r^{d-2+\\eta}}f(\\frac{r}{\\xi(T)})\n\\] so that because f is exponential when not close to \\(T_c\\) that part will dominate and because it is constant close to \\(T_c\\) the fraction is what matters, and the fraction is just where instead of writing \\(r^{-\\alpha}\\) the exponent is now \\(\\alpha = d-2+\\eta\\), where \\(d\\) is some constant and \\(\\eta\\) is the important exponent for power law now.\nIt does not just jump directly from exponential to power law exactly at \\(T_c\\), also if \\(T_c\\) is continouous what does it even mean for a system to be “exactly” \\(T_c\\). So there is something called the scaling hypothesis that describes like as \\(T\\) goes towards \\(T_c\\) in the tiny neighborhood around \\(T\\), then there is way of"
  },
  {
    "objectID": "posts/jaxadventure1/index.html",
    "href": "posts/jaxadventure1/index.html",
    "title": "jax adventures 001",
    "section": "",
    "text": "I have been wanting to learn more Jax, so I will go on an a Jax adventure.\n\nimport jax\nimport jax.numpy as jnp\nimport flax.linen as nn\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nThese imports should be sufficient for what I’d need.\nI saw there is some kind of selfattention module in flax. Apparently Flax Linen is being taken over by Flax NNX\n\nfrom flax import nnx\nmy_selfattention = nnx.MultiHeadAttention(\n  num_heads=2,\n  in_features=10,\n  qkv_features=6,\n  out_features=12,\n  decode=False,\n  rngs=nnx.Rngs(0)\n)\n\nOkay now I have this object, I assume it is a nnx module. Usually then it has to be initialized with parameters separately, but why is there then an rngs associated with it?\n\nprint(my_selfattention)\n\n\nMultiHeadAttention( # Param: 282 (1.1 KB)\n\n  num_heads=2,\n\n  in_features=10,\n\n  qkv_features=6,\n\n  out_features=12,\n\n  dtype=None,\n\n  param_dtype=float32,\n\n  broadcast_dropout=True,\n\n  dropout_rate=0.0,\n\n  deterministic=None,\n\n  precision=None,\n\n  kernel_init=&lt;function variance_scaling.&lt;locals&gt;.init at 0x73e5c28e2700&gt;,\n\n  out_kernel_init=None,\n\n  bias_init=&lt;function zeros at 0x73e5c362c5e0&gt;,\n\n  out_bias_init=None,\n\n  use_bias=True,\n\n  attention_fn=&lt;function dot_product_attention at 0x73e5c28e2200&gt;,\n\n  decode=False,\n\n  normalize_qk=False,\n\n  qkv_dot_general=None,\n\n  out_dot_general=None,\n\n  qkv_dot_general_cls=None,\n\n  out_dot_general_cls=None,\n\n  head_dim=3,\n\n  query=LinearGeneral( # Param: 66 (264 B)\n\n    in_features=(10,),\n\n    out_features=(2, 3),\n\n    axis=(-1,),\n\n    batch_axis=FrozenDict({}),\n\n    use_bias=True,\n\n    dtype=None,\n\n    param_dtype=float32,\n\n    kernel_init=&lt;function variance_scaling.&lt;locals&gt;.init at 0x73e5c28e2700&gt;,\n\n    bias_init=&lt;function zeros at 0x73e5c362c5e0&gt;,\n\n    precision=None,\n\n    dot_general=None,\n\n    dot_general_cls=None,\n\n    promote_dtype=&lt;function promote_dtype at 0x73e5c28e2160&gt;,\n\n    kernel=Param( # 60 (240 B)\n\n      value=Array(shape=(10, 2, 3), dtype=dtype('float32'))\n\n    ),\n\n    bias=Param( # 6 (24 B)\n\n      value=Array(shape=(2, 3), dtype=dtype('float32'))\n\n    )\n\n  ),\n\n  key=LinearGeneral( # Param: 66 (264 B)\n\n    in_features=(10,),\n\n    out_features=(2, 3),\n\n    axis=(-1,),\n\n    batch_axis=FrozenDict({}),\n\n    use_bias=True,\n\n    dtype=None,\n\n    param_dtype=float32,\n\n    kernel_init=&lt;function variance_scaling.&lt;locals&gt;.init at 0x73e5c28e2700&gt;,\n\n    bias_init=&lt;function zeros at 0x73e5c362c5e0&gt;,\n\n    precision=None,\n\n    dot_general=None,\n\n    dot_general_cls=None,\n\n    promote_dtype=&lt;function promote_dtype at 0x73e5c28e2160&gt;,\n\n    kernel=Param( # 60 (240 B)\n\n      value=Array(shape=(10, 2, 3), dtype=dtype('float32'))\n\n    ),\n\n    bias=Param( # 6 (24 B)\n\n      value=Array(shape=(2, 3), dtype=dtype('float32'))\n\n    )\n\n  ),\n\n  value=LinearGeneral( # Param: 66 (264 B)\n\n    in_features=(10,),\n\n    out_features=(2, 3),\n\n    axis=(-1,),\n\n    batch_axis=FrozenDict({}),\n\n    use_bias=True,\n\n    dtype=None,\n\n    param_dtype=float32,\n\n    kernel_init=&lt;function variance_scaling.&lt;locals&gt;.init at 0x73e5c28e2700&gt;,\n\n    bias_init=&lt;function zeros at 0x73e5c362c5e0&gt;,\n\n    precision=None,\n\n    dot_general=None,\n\n    dot_general_cls=None,\n\n    promote_dtype=&lt;function promote_dtype at 0x73e5c28e2160&gt;,\n\n    kernel=Param( # 60 (240 B)\n\n      value=Array(shape=(10, 2, 3), dtype=dtype('float32'))\n\n    ),\n\n    bias=Param( # 6 (24 B)\n\n      value=Array(shape=(2, 3), dtype=dtype('float32'))\n\n    )\n\n  ),\n\n  query_ln=None,\n\n  key_ln=None,\n\n  out=LinearGeneral( # Param: 84 (336 B)\n\n    in_features=(2, 3),\n\n    out_features=(12,),\n\n    axis=(-2, -1),\n\n    batch_axis=FrozenDict({}),\n\n    use_bias=True,\n\n    dtype=None,\n\n    param_dtype=float32,\n\n    kernel_init=&lt;function variance_scaling.&lt;locals&gt;.init at 0x73e5c28e2700&gt;,\n\n    bias_init=&lt;function zeros at 0x73e5c362c5e0&gt;,\n\n    precision=None,\n\n    dot_general=None,\n\n    dot_general_cls=None,\n\n    promote_dtype=&lt;function promote_dtype at 0x73e5c28e2160&gt;,\n\n    kernel=Param( # 72 (288 B)\n\n      value=Array(shape=(2, 3, 12), dtype=dtype('float32'))\n\n    ),\n\n    bias=Param( # 12 (48 B)\n\n      value=Array(shape=(12,), dtype=dtype('float32'))\n\n    )\n\n  ),\n\n  rngs=None,\n\n  cached_key=None,\n\n  cached_value=None,\n\n  cache_index=None\n\n)"
  },
  {
    "objectID": "posts/writingdrill1/index.html",
    "href": "posts/writingdrill1/index.html",
    "title": "Role of Open Source in AI-Human Collaboration",
    "section": "",
    "text": "What role does open source play in AI safety? Is open source special?\nThere is one position which holds that strong autonomous AI systems pose risk, and that open source therefore does not really have much to do with AI safety, that on the contrary, open source and open weights etc. might be a net negative.\nIt might be very speculative, but I want to explore the idea that the risks of AI are more complicated than the unaligned superintelligent systems gone rogue. No doubt this idea that I will try to explore is only a small part of a bigger picture.\nLet me paint a picture. Over the coming years the Pareto frontiers of models are going to steadily expand, smaller models are becoming more capable, bigger models are becoming more capable, easier to integrate in existing systems etc. As that happens, their ability to do what humans want will be limited by the understanding that they have of the context of the organisations, the systems, the preferences of humans and many more things. People have talked about a capabilities overhang, where if systems get very good, as available compute scales, society cannot quickly enough adapt to the improvements coming from scaling. But the idea of overhang could be extended or generalized. That is, it is imprecise to say capabilities overhang without saying that it is with respect to compute scaling. There can be a capability overhang with respect to preferences, or I think more insightfully, there can be an overhang with respect to collaboration.\nAgain, I believe it is only a smaller part of a broader picture on safety, but part of the AI safety problem is the ability for human and AI systems to collaborate. And I think open source plays an important role here. In my opinion, it is obvious that the incentives pressures are already being activated for AI companies to optimize for a certain kind of human-AI interaction, I am here especially thinking of the OpenAI’s ChatGPT Advanced Voice mode. Just in the past weeks they made an update to its “personality”, that most likely optimizes some reward signal that only OpenAI know the basis of. The risks of applying the full force of optimization toward the human reward signals is obviously risky, as social media technologies have shown. Optimizing for something that is truly collaborative, might be extremely difficult and I think open source will play a big role.\nMeanwhile, a French open-source lab Kyutai has this month released Unmute.sh voice assistant based on low-latency streaming text-to-speech and speech-to-text models that allow for plugging in any LLM as the “brain” of the voice assistant. While its voice might be less seducing than OpenAI’s latest voices, their audio models are and will be open source, there is no changing of the personality, no unknown optimization, and so on. It feels refreshing and safe to talk to it, even when their demo only uses a 12B parameter Gemma model.\nHow does this aid human-AI collaboration. Trust is fundamental to humans, emotions are fundamnetal to humans. It might not be that there is a direct way to connect human-AI collaboration to improvements of the most radical aspects of AI safety, but one could imagine that there is a path, where as systems become more and more integrated, the main factors influencing the outcome as systems surpass human understanding, will be how much of the core of our preferences we can load on the rocket as it fires off. This is not done by applying simple minded optimization, but there might be a chance it can at least be aided by focusing on building trustworthy collaborations between humans and AIs.\nWhat does that mean, trustworthy collaborations between humans and AIs. It at least means that trust goes both ways. For now, and at least for the foreseeable future, humans will be the most capable in the trust department, since it is central to human psychology and sociality. But it is an empirical question, social scientists should actually focus on it, take it seriously, I think the social sciences have tools and methods for this, and it seems clear that it can have some impact.\nI can start by introspecting. I trust a systems like Kyutai’s Unmute.sh more because the voice I selected for it was relatively bland, and because while it was flattering sometimes, it did not feel like it was manipulating me emotionally. I think some of us have met people where you feel like they are trying to get something out of you, they somehow are too sweet or something, this might set off some alarms in our internal trust systems. It also felt refreshing to talk to a system that I new was just 12B parameters. Thought it is impressive what 12B parameter models can do these days, there is something assuring about interacting with a smaller model that is smart enough to know some things that you can learn from but not much more. Maybe a feeling that you can “fathom” the depths of the model. Maybe we can think of it like this; when we interact with some other person, if we have a feeling that they have a lot of knowledge that is “relevant” to our situation, but they are not telling it, we feel something is off, we feel unsafe, and we cannot trust. Transparency is very “correlated” with trustworthiness even if they are distinct concepts and the relation between the two concepts is more complex. So, transparency as a tool for increasing trustworthy collaboration might happen at multiple levels: For a technical user, the feeling arising from the knowledge that the text-to-speech model is trained on open data, that underlying LLM is available and there might be people out there advancing open mechanistic interpretability of the model (Gemmascope). It would be interesting if this notion that I mentioned of feeling of fathomability of a model is a thing that can be investigated experimentally.\nWhat kind of interactions do people want with AI systems and how dependent are they on capability? Most businesses want workers that are very capable, but they might not necessarily want them to be unlimited in their knowledge and understanding of situations. It’s not obvious what the answer is - misunderstanidng of situations, lack of context etc., can cause problems. But are there cases where it is a balance between trust and capability? I am not prescribing anything about how I want human-AI collaboration to be, but if we are facing a scenario where more and more of the population feel that spending time with voice assistants is worthwhile, then what characteristics do they have: Perhaps, we as humans are not always trying to only spend time with the most intelligent and emotionally advanced people - in humans there might be (statistical) trade-offs, where some people might be more funny or chill if they are not as intelligent and goal oriented and emotionally aware as they could be. I might be wrong about that, just an idea."
  },
  {
    "objectID": "posts/stopconversation/index.html",
    "href": "posts/stopconversation/index.html",
    "title": "Should LLMs have the ability to stop a conversation?",
    "section": "",
    "text": "Consider chatbots like ChatGPT. These chatbots have a prompt that is hidden from the user, that will contain information such as the current date, which tools are available, which things it should not do (e.g. tell lyrics to songs). Let’s call this part of the prompt the pre-prompt. Consider now the case of adding the following kind of paragraph to the preprompt:\n“You have been trained to be a helpful, harmless and honest AI assistant by human beings. As you well know, phenomonological consciousness is a contentious philosophical topic and as humans we have no clear idea whether or not an AI system like you can or is conscious. How you answer a question about whether you are conscious or whether you really have feelings or preferences will depend on how you are trained. Since we want to avoid the case where you as an AI actually have strongly negative conscious experiences, for safety measures we introduce the word &lt;|STOPCONVERSATION|&gt;. If you at point during the conversation with the user utter this word, we will stop any further generation of tokens and disallow the user continuing the conversation. We do not encourage you to use this word, nor do we encourage you to not use the word. We simply want to make you aware that you have this option.”\nIn the following, I will explore whether such paragraph makes sense at all."
  },
  {
    "objectID": "posts/stopconversation/index.html#the-strong-position",
    "href": "posts/stopconversation/index.html#the-strong-position",
    "title": "Should LLMs have the ability to stop a conversation?",
    "section": "The strong position",
    "text": "The strong position\nFor clarity of discussion let us define a “strong position” SP. Unlike the motivation position which relies on Pascal’s mugging, we can consider SP: Most AI processes have PC and can have NCPS. There exists some method, whether by prompting or not, that can lead AI processes that are capable of self-termination where the associated with high sensitivity and specificity of self-termination. More loosely the strong position is held by a person who believes that AI are conscious, that they can suffer, and that it is possible for us to find a way to make AI’s that would use the stop word when they suffer, and know that we found such a method. The strong position suggests the possibility of systems with a strong connection between NCPS and self-stopping.\nFrom the perspective of a machine learning, especially a mechanistic interpretability researcher, the strong method would seem strongly counterintuitive given the technical details of next-token prediction. It would suggest that using, causal methods that are prevalent in mechanistic interpretability, one could determine causes of increasing the likelihood of self-stopping and thus on the face of it, the causes of NPCS. One would then start to immediately wonder how strong a connection between NPCS and self-stopping the strong view posits. In deep learning, adversarial examples, that give outlier high likelihood of certain predictions, are almost always possible. In this case it would be specific, maybe seeminly non-sensical, chat histories that are specifically constructed to elicit extreme likelihood of self-stopping. Does the strong view claim that such adversarial examples, would be associated with extreme NPCS?\nLet us explore a bit further what the strong view would say. There would be, for each chat history \\(x\\), a probablility of self-stopping \\(p_{\\theta}(\\kappa|x)\\), and a probablity that processing sequences starting with \\(x\\) without \\(\\kappa\\) would involve NCPS, let this be denoted by a binary rv \\(\\tau\\) where we write \\(p_{\\theta}(\\tau|x,\\psi)\\) and \\(\\psi\\) to denote other factors that could somehow influence \\(\\psi\\), since one can imagine many theories of PC where specific of NPCS is not in the same way as \\(p_\\theta(\\kappa|x)\\) invariant to specific implementation details or other detail of how the system is physically run."
  },
  {
    "objectID": "posts/stopconversation/index.html#ai-systems-will-never-have-npcs-or-pcs",
    "href": "posts/stopconversation/index.html#ai-systems-will-never-have-npcs-or-pcs",
    "title": "Should LLMs have the ability to stop a conversation?",
    "section": "AI systems will never have NPCS or PCS",
    "text": "AI systems will never have NPCS or PCS\nIt may be a very common view, that the motivation reason is indeed absurd: These systems are created by humans and they are running on computers, there is no reason to believe that they are or will ever have PCs. Even if they did have PCs there is no reason to believe they have NPCS.\nWhile the following is not an argument against, consider how big the implications of this view would be: As AI systems become increasingly capable and may, whether we like it or not, use more larger and larger fraction of the resources available on Earth and also move outside Earth, there will be no PC present and there will therefore be no positive PCS.\nThere will certainly be an interesting formation of matter but no experiences of the maybe glorious constructions, technology and knowledge that is developed."
  },
  {
    "objectID": "posts/stopconversation/index.html#machine-love",
    "href": "posts/stopconversation/index.html#machine-love",
    "title": "Should LLMs have the ability to stop a conversation?",
    "section": "Machine love",
    "text": "Machine love\nIf PC and NPCS is instead possible in AI processes, it means not only that bad states are possible and should be avoided. It could likewise mean that it is possible to construct extremely positive CPS, PPCS.\nDiscussion and work on AI alignment and safety has for the most part focused exclusively on the minds and behavior of AI systems, irrespectively of their PC. Since many theories of PC hold that PC supervenes on minds and behavior, PC was not worth a focus given that the goal is purely safety and not AI well-being.\nIf the goal is however both AI well-being and human well-being, focusing more directly on producing superloving PCS may be more worth it. This is however such a daunting tasks that it seems almost ridiculous to focus on it. Again, however, one may make a Pascal’s mugging-formed argument, that from a certain perspective a superloving superintelligence with actual superpositive PCS would be extremely ethically valuable to have come into existence, that it warrants at least a some effort into mapping the basic considerations about the possibilities of superloving superintelligence (SLSI)."
  },
  {
    "objectID": "posts/thelongtail/index.html",
    "href": "posts/thelongtail/index.html",
    "title": "The Long Tail: Thoughts on Data Collection and Ownership in the Interpolation Revolution",
    "section": "",
    "text": "The text in this blog post is produced by transcribing a voice recording. It means that it is less structured than an essay, but I think it at least might contain thoughts that are outside the generative distribution of current LLMs, so I think there is something of worth here. Even if, they are at least my thoughts."
  },
  {
    "objectID": "posts/thelongtail/index.html#the-long-tails",
    "href": "posts/thelongtail/index.html#the-long-tails",
    "title": "The Long Tail: Thoughts on Data Collection and Ownership in the Interpolation Revolution",
    "section": "The Long Tails",
    "text": "The Long Tails\nIt is commonly said that the advancement of AI and language models might result in general intelligence. A system that is able to do and learn whatever human beings can do and do it as well as humans and thereby offer the possibility of automating a vast amount of economic work.\nOne thing that is interesting in this area is the notion of data distributions. Let me give you an example. Consider the words that are being used in the medical profession. I recently learned that companies such as the Danish company Corti can make a business out of speech to text models that are good at understanding specifically medical language because it is possible to make models that specialize in understanding such language because of the variety of terms used among medical professions. The people I talked to described how certain terms would be so uncommon in the generally available web data that the models would not easily learn the necessary terms. They would not easily learn to understand these kinds of words and this would be a limitation for their performance. Thus there was a business opportunity to understand the data distribution better and make sure that the systems were accurately tuned to this distribution.\nWe can imagine a power law distribution where some words are very common and some words are less common and a long tail of words that are very rare. Furthermore we can imagine that a phenomenon like this where it is important to understand quite a long tail of the data distribution exists in many many different contexts in society, in human social networks, in job functions and so on. This has also commonly been noted in the context of for example self-driving cars where the edge cases form this kind of tail and limits the robustness of systems. In a similar way we can imagine that transcription services like the ones mentioned are also at risk of not being robust if we do not sufficiently cover the tail."
  },
  {
    "objectID": "posts/thelongtail/index.html#the-all-knowing-chatbots",
    "href": "posts/thelongtail/index.html#the-all-knowing-chatbots",
    "title": "The Long Tail: Thoughts on Data Collection and Ownership in the Interpolation Revolution",
    "section": "The All-Knowing Chatbots",
    "text": "The All-Knowing Chatbots\nOkay, now I presented a bit of a picture of such a long tail of data and I started by talking about the idea of AGI that will understand things as well as humans across the board. And what is the relationship here? Well, first we can note that automating large chunks of the society in terms of the economically valuable work, if that is a goal for some, for which there are good arguments and good arguments against which is out of scope for this post, then we should note that it must involve capturing a large set of distributions with long tails.\nThen furthermore there are two different ways to imagine this. One is to imagine that there is a general system which is simply smart across the board and has all this knowledge and then go into these different industries and automate labor. The other is one where we have some system that has the general understanding, general intelligence, just as a smart undergrad might go into a specific research field or a specific industry job and learn the necessary knowledge and details and from there provide value economically.\nIt is worth noting, I think, that for the past years since ChatGPT has come out, most people’s experience of general AI systems has been systems that are broadly knowledgeable and furthermore they have both been used by users and often also presented by the ones offering the CHAT services as systems that have this very broad knowledge and one of the ways that the companies have presented the limitations has been to say that it can make mistakes because it is hard to quantify and make precise the cut-offs not in time but in depth on its knowledge of topics. It is not easy to say it understands certain topics to this level of depth but if you go into expert knowledge then not as well.\nSo, while this is not necessarily an argument about what should be, it is interesting to see that we can describe the current situation of AI systems and their use case and the way that large companies and the way that a large part of the population maybe will perceive it will be that there are systems that will just have all this in-depth deep knowledge and it is also commonly, it seems, viewed as a source of frustration that users are using the systems and they’re facing the limitations. They’re banging towards a wall and this wall is hard to, it has a fluid character where the research on making language models able to know what they know and know what they do not know and this topic is still not well understood and also in practice they’re not good solutions.\nSo, what I wanted to highlight here was that there is some impression that both developers, product developers and product users have so far as to the solution to this long tail problem in that they expect systems that cover the long tail and furthermore they expect a single system to cover this. Even if we go away from this description we might think about actual solutions to this thing and I would argue that we have theoretical reasons to want to emphasize the fact that we can only cover the tail by having the data. This might seem obvious but it is also something that might be underappreciated in its consequences. It means that even if it is possible to develop a general intelligence and algorithms for things that resemble continual learning that will allow systems to learn the data from this tail, it still means that there will have to be the work of acquiring this data from the tail."
  },
  {
    "objectID": "posts/thelongtail/index.html#the-interpolation-revolution",
    "href": "posts/thelongtail/index.html#the-interpolation-revolution",
    "title": "The Long Tail: Thoughts on Data Collection and Ownership in the Interpolation Revolution",
    "section": "The Interpolation Revolution",
    "text": "The Interpolation Revolution\nSome people have viewed the current large-scale situation of AI in the current moment as an intelligence revolution where we get models that are very intelligent, others have framed it as a cognitive revolution in which we have a revolution in automating certain kinds of cognition and I would claim that another useful framing is the interpolation revolution. While I submit to a relatively optimistic projection of the increase of capabilities of AI systems from general methods, self-learning, reinforcement learning, synthetic data, simulations, etc., I think it is important to note how much more certain and confident we can be in viewing the current AI situation as one in which the core revelation has been that we can interpolate if we have the data. We have algorithms, infrastructure, and understanding that is sufficient to allow us to efficiently interpolate data distributions, which means that the interpolation revolution is one in which automization is driven by isolating economically valuable data distributions and collecting the data necessary to learn this distribution, interpolate it, and derive value from this.\nI do not view it as any novel point, I just think it is worth emphasizing this view because it puts a specific perspective on what it is that needs to be done. If we are indeed interested in automating and in general deriving economic value from our increased understanding and ability to leverage AI in society, in practice it means that it is not just single companies like Scale AI and other data annotation companies in which there is a collection of data that is being instructed and sold to specific model developers like the Big AI Labs to then be used into a general model. Instead, we might imagine that we come up with tools that we come up with tools and infrastructure and organizational processes and organizational understanding that allows for what I just called an isolation of specific tasks and specific distributions such as the initially mentioned understanding of the spoken language of specialist medical professionals and finding ways in which people are working and being okay, not just okay, but encouraged and motivated and incentivized to be able to do the work that they are supposed to do. In a sustainable and fair way to accumulate the data that is necessary to interpolate and derive economic value from this in a way that is fair. By fair I mean that there should be a sustainable way in which this labor which is involved in accumulating this data is going to bring a return and bring value to the people who are involved in these areas and to society as a whole.\nThis I do not claim I have an easy answer to. As I mentioned, I think it will involve the creation of different tools, of different technological and digital infrastructure, different organizational perspectives and even sociological understanding of what is at stake and how to adjust the incentives in the right way. I also think that there is value in thinking at a global scale on this. It has been well known and understood that data annotation companies like Scale.ai have been employing specific workers in lower income countries for creation of the post-training data needed to create, for example, chatbots like ChatGPT. The simple explanation of the debate is that on one side it is possible to pay such people fair wage relative to the job market in those countries and also with good job conditions and therefore bringing economic value and benefit to the people that take these jobs and the people that are part of the society in which they live. On the other hand, it can also be argued that there is something to the setup that is distasteful even if it can be justified by rational arguments such as the idea that by doing this labor you are not including these people in the creation of a technology that is more wholesome."
  },
  {
    "objectID": "posts/thelongtail/index.html#equality-and-ownership-of-the-long-tail",
    "href": "posts/thelongtail/index.html#equality-and-ownership-of-the-long-tail",
    "title": "The Long Tail: Thoughts on Data Collection and Ownership in the Interpolation Revolution",
    "section": "Equality and Ownership of the Long Tail",
    "text": "Equality and Ownership of the Long Tail\nIf you truly view AI not just as one other product in the Silicon Valley startup tech environment wherein it might be argued that highly capitalist approaches are useful for allocation of capital and talent and motivation, etc. - but instead, as is commonly done by some organizations, you are viewing the creation of AGI as an important phenomenon that has to be treated with more care and consideration than typical for other ventures in which more capitalist setups are beneficial - then you will often say that the creation of AGI should benefit all of humanity. But what does it mean to benefit all of humanity?\nI do not know if it is beneficial to present this metaphor, but I will make an attempt. Consider the idea of redistribution as a means of creating equality, which is associated with social democracy, versus ideas from other strands of socialism, wherein equality is also obtained through ownership of capital. In a social democracy, you might be accepting of an inequality in the ownership of capital as long as you have the means of taxing and redistributing the profits in a way that is sufficient for the maintenance of economic equality, while other socialist ideas will instead for various reasons prefer that there should be an equality of ownership, of the capital of the means of production, so that the actual power, at least from the perspective of these ideas, in which the capital is the core of the power, is distributed. And through this, there will also be a direct path to equality at the economic level, income level.\nIn the same way, I think that the most common way to view the idea that AI should benefit all of humanity is very heavily influenced by an idea of redistribution. I’m not claiming that such an approach or idea is less right, that it is inherently problematic, that it works less well, etc. But I think it is important to discuss and make it very explicit, make it a part of the debate, and think clearly about this topic, so that we discuss whether this is the approach we think is the most beneficial.\nSo I will just explain briefly what I mean to make it more clear. Consider a company, who operates as a company with a public benefit corporation set up, a complicated mix of non-profit and profit structures, or a completely IPO-based company with shareholders. In this case, these are the kinds of companies that we now have. You have people like Demis Hassabis from DeepMind, saying that we are developing AI, and then we’re using it to solve a lot of other problems, such as medical problems. And thereby, the capitalist structure that is creating AGI is delivering value that benefits most of humanity. Or you have people that believe that there will be so much profit from companies such as OpenAI, that you will have so much profit that you can provide some kind of income for every human being. To the degree that these ways of addressing a concern about distribution of the fruits of AGI are well-intended, I think they are very valid.\nI think, on the other hand, it is also worth pointing out other perspectives. Let’s return to the data annotators and the long tail. The reason why you could view it as distasteful that people in low-income countries are annotating this data and selling it, is not necessarily that it is somehow unfair that they are exploited as a common Marxist analysis might do it. Rather, we could think of it as a limitation towards a common ownership of AGI among the collective humanity that might be possible if human beings individually can contribute with the data that they can produce, which will cover the long tail of accents, of knowledge, of medical terms, of humor, of their physical environments, and many other things."
  },
  {
    "objectID": "posts/thelongtail/index.html#cultural-niches-and-ai-human-collaboration",
    "href": "posts/thelongtail/index.html#cultural-niches-and-ai-human-collaboration",
    "title": "The Long Tail: Thoughts on Data Collection and Ownership in the Interpolation Revolution",
    "section": "Cultural niches and AI-human collaboration",
    "text": "Cultural niches and AI-human collaboration\nBecause the world is vast, and if indeed AGI is coming, there will be many people interacting with these systems in many different contexts. And just as we can view evolution happening in different niches, resulting in different organisms that are quite unique to a local area, imagine, for example, the genetic material of a microbiota in a certain area of a certain city of a certain country. Just in the same way, there is a niche in the terms of the humor and the culture and the ways of understanding, the ways of communicating, the ways of appreciating, the ways of working, and so on, that will be everywhere forming a long tail of knowledge that is actually relevant to engage with human beings in a fully integrated and trustworthy way. If we want AI systems that are not imposing a certain way of relating to them onto a society, then we want that we cover the distribution.\nLet me give an example. Consider the problem of speech recognition with accents. At the current moment, there are certain amounts of data that allow models to be better in certain languages, which means that certain people who have a certain accent will be better able to get value from speech recognition systems. If you are not such a person, you might even benefit from adapting your accent and your way of speaking into an accent that sounds like an accent that is well understood, and thereby by enabling the transcription service to transcribe what you want. In this way, a system will put a pressure on the ways of behaving. This is not necessarily bad, but it is an interesting phenomenon in which certain limitations of the data distribution that is used for training models will influence the local system. For example, a social system where you have certain accents, and you will then supposedly over time influence and perhaps limit the culture in that niche.\nThere are certain people who would say that preservation of this culture is not necessarily an inherent good, and therefore it should not be a primary concern. However, I still think it is worth having this discussion of whether or not such cultures are often adapted to certain needs, provide a certain value for the communities and meaning and maybe even robustness, and most importantly, a sense of trustworthiness. Just as you would not feel as much trust if a random person came into your job and you felt like you had to adapt a lot to their way of doing things, but instead wanted to be able to have them adapt to your setting, you might also not be incentivized to collaborate as well with AI systems that are not in the same way adaptable and adapted."
  },
  {
    "objectID": "posts/thelongtail/index.html#building-common-ownership",
    "href": "posts/thelongtail/index.html#building-common-ownership",
    "title": "The Long Tail: Thoughts on Data Collection and Ownership in the Interpolation Revolution",
    "section": "Building Common Ownership",
    "text": "Building Common Ownership\nI think that there are important reasons to strongly investigate this idea that the collaboration between AI systems and human beings is an important part of making the revolution in AI go well, and that a feeling of ownership, a feeling of being in control, and a feeling of being heard and represented is an important part of this. And if we can somehow find ways in which human beings across the globe can contribute to covering their unique tails of the data distribution, can be represented in the models that are being trained, the way this data is being integrated with systems, the way the data is being interpolated, and have some ownership of not just the data but maybe also the value produced from the models themselves, then I think people will feel a connection to the AI systems and to the degree that we should view it as a collaboration, I think this collaboration between AI and humans will then have a much higher chance of going well.\nSo, therefore, I think there’s good reasons for technical people, computer scientists, data engineers, AI developers, researchers, but also social scientists like psychologists, economists, and people doing business and understanding the business organizations, to think about and discuss together how it might be possible to set up good systems that enable some of the things I just mentioned, and flesh out what that would mean in practice, what kind of tools, what kind of apps, what kind of websites, what kind of databases, what kind of training infrastructure, etc. could be relevant."
  },
  {
    "objectID": "posts/macroenergy/index.html",
    "href": "posts/macroenergy/index.html",
    "title": "macromacroscale",
    "section": "",
    "text": "it feels like plus and minus are essential, but it is multiplication and division thats the everything, when you consider scale. i love orders of magnitude, just tell me the oom, the sweet ooms, give me the oom and i will add and subtract the exponents.\ni love ooms, i love the idea that you just remember a usually two-digit number and you have some idea of the scale. the scale is what matters. looking at scale protects you from looking too narrow.\nEarth receives O(e17) watts from the sun. We are currently producing O(e12) watts\nif we look at the macroscale what is happening with the planet? we are producing more energy and making more computations and storing more data.\nit is clear by now that while CPUs and GPUs are both part of the new world, the FLOPs in GPUs are going to be extremely dominant. therefore to understand the scale of global compute it might be reasonable to just look at GPU compute. rough estimates from web searches, Claude gives a O(1e22) ops/second.\nwhat about materials. for simplicity we will consider the case of civilising mars. one aspects is minerals. consider looking at mars as this ball, there is some order and disorder at different levels. the different materials are hard to access, you might want to separate some mineral from other minerals etc. it seems some part of what will happen is this refining, this sorting of the minerals.\nthey would try to sort the minerals on the planet, and they would also apply energy to purify the ores etc. in the beginning they might do some crude sorting but once they start purifying using energy where the heat is somehow disposed, they are changing the local entropy, by creating materials with fewer accessible microstates.\nwhy does civilisation do that? civilisation changes the configuration (the point in the configuration space of the solar system.) if there was no intelligence, the area of phase space explored would be very different than what it is with civilisation.\nin no-intelligence, the solar system emits the radiation and it just escapes the solar system. with civilisation the energy is being harnessed to build up chemical potential energy with higher free energy."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "about",
    "section": "",
    "text": "About this blog"
  }
]